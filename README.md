# Izumi

| GitHub | NPM | Documentation | License |
| ------ | --- | ------------- | ------- |
| [![GitHub](https://img.shields.io/badge/GitHub-izumi-blue?logo=github)](https://github.com/getakita/izumi) | [![NPM](https://img.shields.io/npm/v/izumi?logo=npm)](https://www.npmjs.com/package/izumi) | [![Documentation](https://img.shields.io/badge/Documentation-izumi-blue?logo=read-the-docs)](https://izumi.dev/docs/) | [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) |

Izumi is an Apache-licensed open-source TypeScript RAG (Retrieval-Augmented Generation) framework for SQL generation with support for multiple LLM providers and Drizzle ORM integration.



## How Izumi works

Izumi works in two easy steps - train a RAG "model" on your data, and then ask questions which will return SQL queries that can be set up to automatically run on your database.

1. **Train a RAG "model" on your data**.
2. **Ask questions**.

If you don't know what RAG is, don't worry -- you don't need to know how this works under the hood to use it. You just need to know that you "train" a model, which stores some metadata and then use it to "ask" questions.


## User Interfaces
These are some of the user interfaces that can be built using Izumi:

- [Node.js/Express Apps](examples/)
- [Next.js Applications](examples/) (comming soon)


## Supported LLMs

- [OpenAI](https://openai.com/) (GPT-4, GPT-3.5-turbo, etc.)
- [Anthropic](https://anthropic.com/) (Claude 3.5 Sonnet, Claude 3 Haiku, etc.)
- [Google](https://ai.google.dev/) (Gemini 1.5 Pro, Gemini 1.5 Flash, etc.)
- [Mistral](https://mistral.ai/) (Mistral Large, Mistral Medium, etc.)
- [Cohere](https://cohere.ai/) (Command R+, Command R, etc.)

All LLM integrations are powered by the [Vercel AI SDK](https://sdk.vercel.ai/) for unified provider support.

## Supported Vector Stores

- [Memory Vector Store](src/vector/MemoryVectorStore.ts) (Built-in, no setup required)
- [PostgreSQL pgvector](src/vector/PgVectorStore.ts) (Production-ready vector similarity search)
- ChromaDB (Coming soon)
- Pinecone (Coming soon)
- Weaviate (Coming soon)
- Qdrant (Coming soon)

## Supported Output Formats

- **Raw SQL** - Standard SQL queries
- **Drizzle ORM** - Type-safe Drizzle query syntax
- **Schema Export** - JSON, TypeScript interfaces, Drizzle schema definitions

## Getting started

### Install
```bash
npm install izumi
```

### Optional Provider Dependencies
Install only the providers you plan to use:

```bash
# OpenAI
npm install @ai-sdk/openai

# Anthropic  
npm install @ai-sdk/anthropic

# Google
npm install @ai-sdk/google

# Mistral
npm install @ai-sdk/mistral

# Cohere
npm install @ai-sdk/cohere

# PostgreSQL pgvector support
npm install pg pgvector
```

### Import
```typescript
// Quick start with factory functions
import { createOpenAIIzumi, createAnthropicIzumi } from 'izumi';

// For custom configurations
import { Izumi } from 'izumi';

// Create an instance
const izumi = createOpenAIIzumi('your-api-key');

// Or with custom config
const izumi = new Izumi({
  llm: {
    provider: 'openai',
    apiKey: 'your-api-key',
    model: 'gpt-4o-mini'
  }
});
## Training
You may or may not need to run these `izumi.train` commands depending on your use case. 

These statements are shown to give you a feel for how it works.

### Train with DDL Statements
DDL statements contain information about the table names, columns, data types, and relationships in your database.

```typescript
await izumi.train({
  ddl: `
    CREATE TABLE IF NOT EXISTS customers (
      id SERIAL PRIMARY KEY,
      name VARCHAR(100),
      email VARCHAR(255) UNIQUE,
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
  `
});
```

### Train with Documentation
Sometimes you may want to add documentation about your business terminology or definitions.

```typescript
await izumi.train({
  documentation: "Our business defines customer lifetime value as the total revenue generated by a customer over their entire relationship with the company..."
});
```

### Train with SQL Examples
You can also add SQL queries to your training data. This is useful if you have some queries already laying around. You can just copy and paste those from your editor to begin generating new SQL.

```typescript
await izumi.train({
  question: "Get all customers from last month",
  sql: "SELECT * FROM customers WHERE created_at >= DATE_TRUNC('month', CURRENT_DATE - INTERVAL '1 month')"
});
```

## Asking questions
```typescript
const result = await izumi.ask("What are the top 10 customers by order total?");
```

You'll get SQL:
```sql
SELECT 
  c.name as customer_name,
  SUM(o.total) as total_orders
FROM customers c
JOIN orders o ON c.id = o.customer_id
GROUP BY c.id, c.name
ORDER BY total_orders DESC
LIMIT 10;
```

And explanation:
```
This query joins the customers and orders tables, groups by customer to calculate the sum of their order totals, and returns the top 10 customers sorted by their total order value.
```

## PostgreSQL pgvector Support

For production workloads, you can use PostgreSQL with the pgvector extension for scalable vector similarity search:

### Setup PostgreSQL with pgvector

```sql
-- Enable the pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;
```

### Configuration

```typescript
import { createOpenAIWithPgVector } from 'izumi';

const pgConfig = {
  host: 'localhost',
  port: 5432,
  database: 'izumi_vectors',
  user: 'postgres',
  password: 'your-password',
  ssl: false,
  schema: 'izumi',
  embeddingDimension: 1536, // OpenAI text-embedding-3-small
  similarityThreshold: 0.7
};

const izumi = createOpenAIWithPgVector(
  process.env.OPENAI_API_KEY,
  pgConfig,
  'gpt-4o-mini'
);

// Initialize tables (run once)
await izumi.initialize();
```

### Features

- **Production-ready**: Built for scale with connection pooling and proper indexing
- **Vector similarity search**: Uses PostgreSQL's pgvector extension for efficient similarity search
- **Automatic schema management**: Creates tables and indexes automatically
- **Embedding support**: Built-in integration with AI SDK for generating embeddings
- **Configurable similarity threshold**: Fine-tune relevance filtering
## RAG vs. Fine-Tuning
**RAG (What Izumi uses)**
- Portable across LLMs
- Easy to remove training data if any of it becomes obsolete
- Much cheaper to run than fine-tuning
- More future-proof -- if a better LLM comes out, you can just swap it out

**Fine-Tuning**
- Good if you need to minimize tokens in the prompt
- Slow to get started
- Expensive to train and run (generally)

## Why Izumi?

1. **TypeScript-first with full type safety.**
   - Built for modern TypeScript/JavaScript applications
   - Full type safety with Drizzle ORM integration
   - Excellent IDE support with IntelliSense

2. **Multiple LLM providers with easy switching.**
   - Start with one provider, switch to another without code changes
   - Unified API across all supported LLM providers
   - Built on the battle-tested Vercel AI SDK

3. **Drizzle ORM integration.**
   - Generate type-safe Drizzle queries, not just raw SQL
   - Export schema definitions in Drizzle format
   - Seamless integration with existing Drizzle projects

4. **Secure and private.**
   - Your database contents are never sent to the LLM or vector database
   - SQL execution happens in your local environment
   - Training data stays within your infrastructure

5. **Self learning.**
   - Automatically improve accuracy with successful query examples
   - Easy feedback integration for continuous improvement
   - Correct question-to-SQL pairs are stored for future reference

6. **Flexible deployment.**
   - Works in Node.js, browsers, serverless functions
   - Easy integration with existing applications
   - Multiple output formats (SQL, Drizzle, explanations)

## Extending Izumi
Izumi is designed to connect to any LLM provider and vector database. There's an [IzumiBase](src/base/IzumiBase.ts) abstract base class that defines the core functionality. The package provides implementations for use with multiple LLM providers via the Vercel AI SDK and includes a memory-based vector store. You can easily extend Izumi to use your own LLM or vector database implementations.

## Examples

### Basic Usage
```typescript
import { createOpenAIIzumi } from 'izumi';

const izumi = createOpenAIIzumi('your-api-key');

// Train with schema
await izumi.train({
  ddl: `CREATE TABLE users (id SERIAL PRIMARY KEY, name VARCHAR(100))`
});

// Ask questions
const result = await izumi.ask('Get all users created today');
console.log(result.sql);
```

### Advanced Configuration
```typescript
import { Izumi } from 'izumi';

const izumi = new Izumi({
  llm: {
    provider: 'anthropic',
    apiKey: process.env.ANTHROPIC_API_KEY,
    model: 'claude-3-5-sonnet-20241022'
  },
  vectorStore: {
    similarityThreshold: 0.8
  }
});
```

## License
This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## More resources
- [Full Documentation](https://izumi.dev/docs/) (Coming soon)
- [GitHub Repository](https://github.com/getakita/izumi)
- [NPM Package](https://www.npmjs.com/package/izumi)

